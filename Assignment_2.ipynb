{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0983b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the answers to the assignment questions. You can put these answers into a Jupyter notebook as required.\n",
    "\n",
    "# ### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "# **Answer:**\n",
    "# - **Euclidean Distance:** Measures the straight-line distance between two points in Euclidean space. Formula: \\( \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} \\).\n",
    "# - **Manhattan Distance:** Measures the distance between two points along axes at right angles. Formula: \\( \\sum_{i=1}^{n} |x_i - y_i| \\).\n",
    "\n",
    "# **Effect on Performance:**\n",
    "# - **Euclidean Distance:** Sensitive to large differences, which can affect performance in high-dimensional spaces or when features have different scales.\n",
    "# - **Manhattan Distance:** More robust to outliers and works well in high-dimensional spaces where data can be sparse.\n",
    "\n",
    "# Choosing between them depends on the data distribution and the nature of the features.\n",
    "\n",
    "# ### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "# **Answer:**\n",
    "# Choosing the optimal value of k is crucial. Techniques include:\n",
    "# - **Cross-validation:** Use k-fold cross-validation to test different k values and select the one with the lowest error rate.\n",
    "# - **Elbow Method:** Plot the error rate against different k values and look for an 'elbow' point where the error rate starts to level off.\n",
    "# - **Domain Knowledge:** Leverage domain-specific insights to select an appropriate k value.\n",
    "\n",
    "# ### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "# **Answer:**\n",
    "# - **Euclidean Distance:** Preferred when the data is continuous and features are on similar scales.\n",
    "# - **Manhattan Distance:** Preferred when dealing with high-dimensional data or when the data includes categorical variables. Itâ€™s also more robust to outliers.\n",
    "\n",
    "# The choice of distance metric affects how neighbors are defined, impacting the overall performance. \n",
    "\n",
    "# ### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "# **Answer:**\n",
    "# Common hyperparameters include:\n",
    "# - **k (number of neighbors):** Determines the number of nearest neighbors to consider.\n",
    "# - **Distance metric:** Euclidean, Manhattan, etc., affecting how distances are calculated.\n",
    "# - **Weights:** Uniform (equal weight) or distance (weight by inverse distance).\n",
    "\n",
    "# **Tuning Techniques:**\n",
    "# - **Grid Search:** Exhaustive search over specified parameter values.\n",
    "# - **Random Search:** Randomly samples hyperparameters.\n",
    "# - **Cross-validation:** Evaluates model performance for different hyperparameter values.\n",
    "\n",
    "# ### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "# **Answer:**\n",
    "# - **Larger Training Set:** Typically improves performance as more data provides more information for neighbor comparisons.\n",
    "# - **Smaller Training Set:** Can lead to overfitting and poor generalization.\n",
    "\n",
    "# **Techniques to Optimize Size:**\n",
    "# - **Dimensionality Reduction:** Use PCA, LDA to reduce the number of features, effectively increasing the relative size of the training set.\n",
    "# - **Feature Selection:** Remove irrelevant or redundant features.\n",
    "\n",
    "# ### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "# **Answer:**\n",
    "# **Drawbacks:**\n",
    "# - **Computationally Expensive:** KNN is slow for large datasets because it requires computing distances to all training samples for each prediction.\n",
    "# - **Sensitive to Irrelevant Features:** High-dimensional data can affect performance due to irrelevant features.\n",
    "# - **Requires Feature Scaling:** Different scales can bias the distance calculation.\n",
    "\n",
    "# **Overcoming Drawbacks:**\n",
    "# - **Efficient Data Structures:** Use KD-trees, Ball-trees to reduce computation.\n",
    "# - **Dimensionality Reduction:** Apply PCA or feature selection methods.\n",
    "# - **Feature Scaling:** Normalize or standardize features to ensure uniform contribution.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
